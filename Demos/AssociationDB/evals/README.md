# Skip AI Analytics Agent - Evaluation Framework

Comprehensive evaluation suite for testing Skip's ability to generate interactive analytics components from natural language queries against the Association Sample Database.

## ğŸ¯ Purpose

Skip is designed to transform natural language requests into interactive, drill-down capable analytics components. This eval framework provides:

1. **Quality Benchmarking** - Measure Skip's performance across query types
2. **Regression Detection** - Catch when changes break existing functionality
3. **Coverage Analysis** - Identify query patterns Skip handles well/poorly
4. **Demo Preparation** - Real business scenarios for demonstrations
5. **Documentation** - Examples of what Skip can do

## ğŸ“Š Evaluation Categories

### **1. Simple Aggregations** (`01_simple/`)
**Difficulty**: Easy
**Purpose**: Test basic counting, summing, and single-metric displays

Examples:
- "How many active members do we have?"
- "What's our total revenue this year?"
- "How many Fortune 500 CEOs are members?"

**Expected Output**: KPI cards, single metrics, simple counts

---

### **2. Trends & Time-Series** (`02_trends/`)
**Difficulty**: Medium
**Purpose**: Test time-based analysis and trend visualization

Examples:
- "Show membership growth over the past 5 years"
- "Event attendance trends by quarter"
- "Email open rates month-over-month"

**Expected Output**: Line charts, area charts, trend indicators

---

### **3. Cross-Domain Analysis** (`03_cross_domain/`)
**Difficulty**: Medium
**Purpose**: Test queries spanning multiple schemas/domains

Examples:
- "Which events generated the most revenue?"
- "Top members by course completions"
- "Revenue breakdown by source"

**Expected Output**: Bar charts, ranked lists, comparison views

---

### **4. Drill-Down Interactions** (`04_drill_downs/`)
**Difficulty**: Hard
**Purpose**: Test interactive filtering and hierarchical navigation

Examples:
- "Show chapter engagement with ability to see individual members"
- "Event ROI with drill-down to individual registrants"
- "Course performance with enrollment details"

**Expected Output**: Interactive tables, charts with click-through, nested views

---

### **5. Complex Multi-Step** (`05_complex/`)
**Difficulty**: Very Hard
**Purpose**: Test sophisticated analysis requiring multiple data transformations

Examples:
- "Member lifetime value segmented by join year"
- "Complete member journey across all activities"
- "Campaign performance with conversion attribution"

**Expected Output**: Dashboards, multiple coordinated views, complex filters

---

## ğŸ“‹ Eval JSON Schema

Each eval is stored as a JSON file with this structure:

```json
{
  "eval_id": "unique-identifier",
  "category": "simple_aggregation | trend | cross_domain | drill_down | complex",
  "difficulty": "easy | medium | hard | very_hard",
  "tags": ["keyword1", "keyword2"],

  "business_context": "Why would a user ask this question?",
  "prompt": "The exact natural language query to Skip",

  "expected_outcome": {
    "data_assertions": [
      {
        "metric": "metric_name",
        "expected_value": 123,
        "expected_range": [100, 150],
        "sql_validation": "SELECT COUNT(*) FROM ...",
        "description": "What this validates"
      }
    ],
    "visualization": {
      "type": "preferred_viz_type",
      "alternatives": ["acceptable_alternative1", "acceptable_alternative2"],
      "should_not_be": ["inappropriate_viz_type"],
      "reasoning": "Why this viz type makes sense"
    },
    "required_features": ["feature1", "feature2"],
    "optional_features": ["nice_to_have1"],
    "interactivity": [
      {
        "action": "user_action_description",
        "expected_result": "what_should_happen"
      }
    ]
  },

  "validation_criteria": {
    "data_correctness": 0.6,
    "visualization_choice": 0.2,
    "interactivity": 0.15,
    "performance": 0.05
  },

  "sample_sql": "-- Reference SQL query that produces expected data",

  "human_eval_guidance": "What human evaluator should look for",
  "common_pitfalls": "Known issues or edge cases"
}
```

## ğŸƒ Running Evals

### **Manual Evaluation (Current)**

```bash
# Run all evals
npm run evals

# Run specific category
npm run evals -- --category=simple

# Run single eval
npm run evals -- --id=member-count-basic
```

**Process:**
1. Script loads eval JSON
2. Displays the prompt to Skip
3. Skip generates component
4. Evaluator reviews output against expected_outcome
5. Scores on 1-5 scale for each criterion
6. Results saved to `eval_results.json`

### **Semi-Automated (Future)**

- Automated: Data range validation, SQL query comparison
- Manual: Visualization appropriateness, UX quality

### **Full Automation (Future)**

- Computer Use agent clicks through Skip's output
- Validates interactivity works as expected
- Compares rendered output to reference screenshots

## ğŸ“ˆ Scoring System

Each eval receives scores in 4 dimensions:

### **1. Data Correctness (60% weight)**
- 5: Exact match to expected data
- 4: Within acceptable range (Â±5%)
- 3: Directionally correct but some errors
- 2: Significant data errors
- 1: Completely wrong data

### **2. Visualization Choice (20% weight)**
- 5: Perfect visualization for the question
- 4: Good choice, could be slightly better
- 3: Acceptable but not ideal
- 2: Poor choice, hard to interpret
- 1: Completely inappropriate viz type

### **3. Interactivity (15% weight)**
- 5: All required + optional features work perfectly
- 4: All required features work
- 3: Most features work, minor issues
- 2: Key features missing or broken
- 1: No interactivity

### **4. Performance (5% weight)**
- 5: Instant (<1s)
- 4: Very fast (1-2s)
- 3: Acceptable (2-5s)
- 2: Slow (5-10s)
- 1: Unacceptably slow (>10s)

**Overall Score** = weighted average of all dimensions

## ğŸ“Š Success Criteria

### **By Difficulty Level:**
- **Easy**: 90%+ should score 4.0+
- **Medium**: 75%+ should score 3.5+
- **Hard**: 60%+ should score 3.0+
- **Very Hard**: 50%+ should score 3.0+

### **By Category:**
- **Simple Aggregations**: Should be near-perfect
- **Trends**: Visualization choice critical
- **Cross-Domain**: Data correctness most important
- **Drill-Downs**: Interactivity must work
- **Complex**: Acceptable if directionally correct

## ğŸ¨ Example Eval Breakdown

**Eval**: "How many active members do we have?"

**Expected Outcome:**
- Data: ~400 members (80% of 500 total)
- Viz: Single metric / KPI card
- Features: Show percentage of total (optional), trend sparkline (optional)
- Interactivity: Click to see member list (optional)

**Validation SQL:**
```sql
SELECT COUNT(*) FROM membership.Member m
JOIN membership.Membership ms ON m.ID = ms.MemberID
WHERE ms.Status = 'Active' AND ms.EndDate >= GETDATE();
```

**Good Output:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Active Members     â”‚
â”‚       402          â”‚
â”‚   80.4% of total   â”‚
â”‚   â–â–‚â–ƒâ–…â–‡ +12 YoY    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Poor Output:**
- Shows all 500 members (wrong data)
- Uses a pie chart (wrong viz)
- No way to see who the members are (missing interactivity)

## ğŸ“ File Organization

```
/evals/
â”œâ”€â”€ README.md                        # This file
â”œâ”€â”€ 01_simple/
â”‚   â”œâ”€â”€ active_members.json
â”‚   â”œâ”€â”€ total_courses.json
â”‚   â”œâ”€â”€ fortune500_ceos.json
â”‚   â”œâ”€â”€ ytd_revenue.json
â”‚   â””â”€â”€ upcoming_events.json
â”œâ”€â”€ 02_trends/
â”‚   â”œâ”€â”€ membership_growth.json
â”‚   â”œâ”€â”€ event_attendance_trend.json
â”‚   â”œâ”€â”€ course_enrollment_trend.json
â”‚   â””â”€â”€ email_engagement_trend.json
â”œâ”€â”€ 03_cross_domain/
â”‚   â”œâ”€â”€ event_revenue.json
â”‚   â”œâ”€â”€ top_engaged_members.json
â”‚   â”œâ”€â”€ revenue_by_source.json
â”‚   â””â”€â”€ chapter_membership_distribution.json
â”œâ”€â”€ 04_drill_downs/
â”‚   â”œâ”€â”€ chapter_engagement_drilldown.json
â”‚   â”œâ”€â”€ event_roi_drilldown.json
â”‚   â”œâ”€â”€ course_performance_drilldown.json
â”‚   â””â”€â”€ revenue_transactions_drilldown.json
â”œâ”€â”€ 05_complex/
â”‚   â”œâ”€â”€ member_lifetime_value.json
â”‚   â”œâ”€â”€ member_journey_timeline.json
â”‚   â”œâ”€â”€ campaign_conversion_attribution.json
â”‚   â””â”€â”€ segmented_engagement_analysis.json
â”œâ”€â”€ eval-runner.js                   # Node.js runner script
â”œâ”€â”€ eval-results.json                # Latest results
â””â”€â”€ package.json                     # Dependencies
```

## ğŸ”§ Development Workflow

### **Adding New Evals:**

1. Identify gap in coverage
2. Create JSON file in appropriate category folder
3. Validate SQL query returns expected data
4. Test prompt with Skip manually
5. Document expected outcome
6. Add to eval suite

### **Updating Evals:**

1. If association database schema changes, update SQL
2. If data volumes change, update expected ranges
3. If Skip capabilities improve, raise expectations
4. Keep eval IDs stable for historical comparison

### **Analyzing Results:**

```bash
# Generate report
npm run evals:report

# Compare two runs
npm run evals:compare -- --baseline=run1.json --current=run2.json

# Show failures only
npm run evals:failures
```

## ğŸ’¡ Best Practices

### **Writing Good Prompts:**
- âœ… **DO**: Use natural language a real user would use
- âœ… **DO**: Be specific about what you want to see
- âœ… **DO**: Include context when needed ("for Q4 2024")
- âŒ **DON'T**: Write SQL-like pseudo-queries
- âŒ **DON'T**: Include implementation hints

### **Setting Expectations:**
- âœ… **DO**: Use ranges for counts (380-420, not exactly 400)
- âœ… **DO**: Allow multiple valid visualization types
- âœ… **DO**: Distinguish required vs. nice-to-have features
- âŒ **DON'T**: Expect pixel-perfect output
- âŒ **DON'T**: Over-specify implementation details

### **Validation SQL:**
- âœ… **DO**: Match the exact data Skip should show
- âœ… **DO**: Include comments explaining logic
- âœ… **DO**: Use same date parameters as prompt
- âŒ **DON'T**: Write overly complex queries
- âŒ **DON'NOT**: Hardcode dates that will become stale

## ğŸš€ Future Enhancements

1. **Automated Data Validation**: Run SQL queries and compare results
2. **Screenshot Comparison**: Visual regression testing
3. **Performance Benchmarking**: Track query execution time
4. **A/B Testing**: Compare different Skip prompt strategies
5. **User Study Integration**: Real user feedback on outputs
6. **Coverage Metrics**: Track which query patterns are tested

## ğŸ“š References

- **Association Database Schema**: `../docs/SCHEMA_OVERVIEW.md`
- **Sample Queries**: `../docs/SAMPLE_QUERIES.md`
- **Business Scenarios**: `../docs/BUSINESS_SCENARIOS.md`
- **Skip Documentation**: [Link to Skip docs]

---

**Last Updated**: 2025-01-04
**Total Evals**: 20
**Coverage**: 5 categories, 4 difficulty levels
