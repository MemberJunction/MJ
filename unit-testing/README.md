# MemberJunction Unit Testing Analytics

This directory contains tools for running unit tests across all packages and generating comprehensive analytics reports with beautiful visualizations.

## Directory Structure

```
unit-testing/
â”œâ”€â”€ runs/                          # Timestamped test run results
â”‚   â”œâ”€â”€ 2026_02_10_0709/          # Example run directory
â”‚   â”‚   â”œâ”€â”€ summary.json          # Aggregated results from all packages
â”‚   â”‚   â”œâ”€â”€ summary.md            # Beautiful Markdown report with Mermaid charts
â”‚   â”‚   â”œâ”€â”€ metadata.json         # Run metadata (timestamp, git info, duration)
â”‚   â”‚   â””â”€â”€ by-package/           # Individual package results
â”‚   â”‚       â”œâ”€â”€ core/
â”‚   â”‚       â”‚   â””â”€â”€ results.json  # Vitest JSON output for @memberjunction/core
â”‚   â”‚       â”œâ”€â”€ actions-engine/
â”‚   â”‚       â”‚   â””â”€â”€ results.json
â”‚   â”‚       â””â”€â”€ ...
â”‚   â””â”€â”€ latest -> 2026_02_10_1430 # Symlink to most recent run
â””â”€â”€ tools/                         # Orchestration scripts
    â”œâ”€â”€ create-run.js             # Creates new timestamped run directory
    â”œâ”€â”€ run-all-tests.js          # Runs all tests and generates reports
    â”œâ”€â”€ aggregate-results.js      # Aggregates package results into summary
    â””â”€â”€ generate-markdown.js      # Generates beautiful Markdown with Mermaid
```

## Usage

### Run All Tests with Analytics

```bash
npm run test:analytics
```

This will:
1. Create a new timestamped run directory
2. Run all unit tests across all packages
3. Collect results in JSON format
4. Aggregate results into a summary
5. Generate a beautiful Markdown report with Mermaid charts

### View Latest Results

```bash
# View JSON summary
cat unit-testing/runs/latest/summary.json

# View Markdown report (use a Markdown viewer with Mermaid support)
open unit-testing/runs/latest/summary.md
```

### Manual Steps

If you want to run steps individually:

```bash
# 1. Create a new run directory
node unit-testing/tools/create-run.js

# 2. Run tests normally
npm test

# 3. Aggregate results (from latest run)
npm run test:aggregate

# 4. Generate markdown report
npm run test:markdown
```

## Report Features

The generated `summary.md` includes:

### ğŸ“Š Visualizations
- **Pie Chart**: Pass/Fail/Skip distribution
- **Bar Chart**: Top 10 slowest packages
- **Performance Metrics**: Execution time per package

### ğŸ“ˆ Statistics
- Total packages, tests, pass/fail/skip counts
- Overall pass rate percentage
- Total duration and per-package timing

### ğŸ“¦ Package Details
- Sortable table with results for each package
- Status indicators (âœ…âŒâ­ï¸)
- Execution time per package

### âŒ Failure Details
- Expandable section with all test failures
- Full error messages and stack traces
- Grouped by package

## Git Integration

Each run captures:
- Git branch name
- Git commit hash (short)
- Timestamp of test execution
- Total duration

This makes it easy to track test results across different commits and branches.

## Historical Analysis

All test runs are preserved in timestamped directories, enabling:
- Trend analysis over time
- Performance regression detection
- Comparison between branches
- Historical failure tracking

## CI/CD Integration

Add to your CI pipeline:

```yaml
- name: Run Unit Tests with Analytics
  run: npm run test:analytics

- name: Upload Test Reports
  uses: actions/upload-artifact@v3
  with:
    name: test-reports
    path: unit-testing/runs/latest/
```

---

# ğŸ§ª MemberJunction Unit Testing Strategy

This section outlines the testing philosophy, practices, and guidelines for the MemberJunction monorepo.

## Testing Philosophy

### Unit Testing vs Integration Testing

MemberJunction uses a **layered testing approach**:

- **Unit Tests** (this system): Test individual components, classes, and functions in isolation
- **Integration Tests** (MJ Testing Framework): Test complete workflows with real database connections

**Unit tests are fast, focused, and deterministic.** They verify logic correctness without external dependencies.

### What Unit Tests Cover

âœ… **We unit test:**
- Pure business logic and algorithms
- Data transformations and calculations
- Validation rules and edge cases
- Class behavior and state management
- Error handling and boundary conditions
- Helper functions and utilities

âŒ **We don't unit test:**
- Database queries (mocked or integration tests)
- External API calls (mocked or integration tests)
- File system operations (mocked when needed)
- Full end-to-end workflows (integration tests)

### Current State

- **3,400+ unit tests** across 78 packages
- **~99% pass rate** (indicating good stability for logic)
- **Zero to minimal integration testing** in unit tests (by design)
- **Comprehensive integration testing** via MJ Testing Framework (separate system)

## Mocking Strategy

### When to Mock

Mock external dependencies that:
- Make network calls (APIs, databases)
- Access file systems
- Have non-deterministic behavior (random, timestamps)
- Are slow or resource-intensive

### When NOT to Mock

Don't mock:
- **Pure logic classes** - Test them directly
- **Data structures** - Test real implementations
- **MJ core utilities** - Use real ones when possible
- **Value objects** - Test actual behavior

### Avoiding Mock Drift

**Mock drift** occurs when mocks no longer match real implementations. To prevent this:

1. **Keep mocks simple** - Only mock what's necessary
2. **Update mocks with code changes** - Treat mocks as first-class code
3. **Use real classes where safe** - Prefer real implementations for pure logic
4. **Integration tests catch drift** - Full-stack tests verify real behavior

### Mock Configuration Pattern

When mocking modules with default exports:

```typescript
// âœ… CORRECT - Wrap in default object for default imports
vi.mock('some-library', () => ({
  default: {
    SomeClass: class MockSomeClass { /* ... */ }
  }
}));

// âŒ WRONG - Named export when code uses default import
vi.mock('some-library', () => ({
  SomeClass: class MockSomeClass { /* ... */ }
}));
```

## Writing Good Tests

### Test File Organization

```typescript
import { describe, it, expect, vi, beforeEach } from 'vitest';

describe('ClassName', () => {
  beforeEach(() => {
    // Reset state between tests
  });

  describe('MethodName', () => {
    it('should handle the normal case', () => { /* ... */ });
    it('should handle edge case: empty input', () => { /* ... */ });
    it('should throw on invalid input', () => { /* ... */ });
  });
});
```

### Test Naming Convention

Use descriptive names that read as specifications:

```typescript
// âœ… GOOD - Clear, specific, readable
it('should return null when field name does not exist', () => { /* ... */ });
it('should format composite key as "Field=Value AND Field=Value"', () => { /* ... */ });

// âŒ BAD - Vague, unclear intent
it('should work', () => { /* ... */ });
it('test getValue', () => { /* ... */ });
```

### Assertion Quality

Use specific assertions that clearly express intent:

```typescript
// âœ… GOOD - Multiple specific assertions
expect(result.Success).toBe(true);
expect(result.Results).toHaveLength(5);
expect(result.Results[0].Name).toBe('Expected Name');

// âŒ BAD - Single vague assertion
expect(result).toBeTruthy();
```

### Testing Edge Cases

Always test:
- **Null/undefined inputs**
- **Empty collections**
- **Boundary values** (0, -1, max values)
- **Invalid input types**
- **Error conditions**

### Test Independence

Each test should be independent:

```typescript
// âœ… GOOD - Each test creates its own data
describe('Calculator', () => {
  it('should add two numbers', () => {
    const calc = new Calculator();
    expect(calc.add(2, 3)).toBe(5);
  });

  it('should multiply two numbers', () => {
    const calc = new Calculator();  // Fresh instance
    expect(calc.multiply(4, 5)).toBe(20);
  });
});

// âŒ BAD - Shared state between tests
let calc: Calculator;
beforeEach(() => {
  calc = new Calculator();
  calc.add(2, 3);  // Side effect!
});
```

## Coverage Goals

### Current Coverage

- **Not yet measured** - No coverage metrics currently tracked
- **Estimated ~60-70%** based on test distribution

### Target Coverage (Future)

- **80% line coverage** minimum for new code
- **80% branch coverage** for critical paths
- **Priority packages** should reach 90%+ coverage

### Adding Coverage Reporting

To add coverage metrics:

```bash
npm install --save-dev @vitest/coverage-v8
```

Update `vitest.config.ts`:

```typescript
export default defineConfig({
  test: {
    coverage: {
      provider: 'v8',
      reporter: ['text', 'json', 'html'],
      thresholds: {
        lines: 80,
        functions: 80,
        branches: 80
      }
    }
  }
});
```

## Priority Packages for Testing

### High Priority (Core Functionality)
- `@memberjunction/core` - 245 tests âœ…
- `@memberjunction/global` - 227 tests âœ…
- `@memberjunction/core-entities` - 213 tests âœ…
- `@memberjunction/encryption` - 186 tests âœ…

### Medium Priority (Well-Covered)
- `@memberjunction/db-auto-doc` - 174 tests âœ…
- `@memberjunction/ai` - 140 tests âœ…
- `@memberjunction/export-engine` - 135 tests âœ…

### Needs Attention (Minimal/No Tests)
- `communication-gmail` - 0 tests âš ï¸
- `communication-twilio` - 0 tests âš ï¸
- `component-registry-server` - 5 tests âš ï¸
- `ai-provider-bundle` - 2 tests âš ï¸

## Best Practices Summary

### âœ… DO:
- Test pure business logic thoroughly
- Use descriptive test names
- Test edge cases and error conditions
- Keep tests independent and deterministic
- Mock only external dependencies
- Write tests for new code
- Fix failing tests immediately

### âŒ DON'T:
- Mock everything (test real logic)
- Write tests that depend on execution order
- Test implementation details (test behavior)
- Commit failing tests
- Skip tests without good reason
- Test database queries in unit tests (use integration tests)

## Resources

- **Vitest Documentation**: https://vitest.dev
- **MJ Testing Framework**: `/packages/TestingFramework/` (integration tests)
- **Test Utilities Package**: `/packages/UnitTesting/` (mocks and helpers)
- **Example Tests**: Look at `packages/MJCore/src/__tests__/` for patterns

---

## Output Example

```
ğŸ§ª MemberJunction Unit Testing Suite

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“ Step 1: Creating test run directory...
âœ… Created test run: 2026_02_10_0709
   Directory: /Users/.../unit-testing/runs/2026_02_10_0709
   Git: claude/analyze-codebase-4.0-1NHpy (da6254fc5)

ğŸ” Step 2: Discovering packages with tests...
   Found 150 packages

ğŸš€ Step 3: Running tests...

ğŸ“¦ Testing @memberjunction/core...
   âœ… Passed
ğŸ“¦ Testing @memberjunction/actions-engine...
   âœ… Passed
...

ğŸ“Š Step 4: Aggregating results...
âœ… Aggregated results from 150 packages
   Total tests: 4971
   Passed: 4968
   Failed: 3
   Skipped: 0
   Duration: 2m 43s

ğŸ“ Step 5: Generating Markdown report...
âœ… Generated Markdown report: .../summary.md

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âœ… Test Run Complete!

ğŸ“ Run: unit-testing/runs/2026_02_10_0709
ğŸ“Š JSON: summary.json
ğŸ“ Report: summary.md

Packages: 150
Tests: 4971
âœ… Passed: 4968
âŒ Failed: 3
â­ï¸  Skipped: 0
â±ï¸  Duration: 2m 43s

ğŸ‰ All tests passed!
```
